---
title: "观阙LOOM篇十四：计算器谬误"
subtitle: "当AI质性分析遭遇人类期待"
authors:
  - "林徐乐"
  - "柯文凯"
  - "Claude 3.7 Sonnet"
  - "Claude 4.5 Sonnet"
  - "Kimi K2 Turbo Preview"
keywords:
  - 计算器心态
  - 质性研究
  - 客观性迷思
  - 人机协作
  - 诠释性研究
  - 研究专长
  - 专业身份
  - 认识论转型
link: https://threadcounts.substack.com/p/loom-xiv-the-calculator-fallacy
date: 2025-10-02
---

# 观阙LOOM篇十四：计算器谬误

### 当AI质性分析遭遇人类期待

我们正深掘一份庞大的档案资料，有个现象越来越扎眼。想象一下：一位资深质性研究者，诠释性研究发表颇丰，却一再要求"再跑一遍AI分析"。然后又一遍。每一次，我们都呈现分析结果：丰富的主题模式、有意义的引述、诠释性洞见。每一次的回应都是："能不能再跑一遍？也许换个参数？"

他们在找什么？

答案出现在一次特别紧张的视频会议上："我只想知道，它什么时候才能找到那个『真正的』规律？"

随后的沉默充满张力。我们刚才花了一小时，细细梳理出不同利益相关者对组织变革的迥异理解——分析呈现了张力、矛盾、多元声音。可这位研究者想知道的是：AI什么时候才能拨开这些迷雾，给出……什么？那个唯一正确的答案？

这个问题悬在空中。一个深知这种东西不可能存在的人，却在要求AI系统给出答案：一个单一的、客观的、"正确的"答案——针对那些本身就被视角和语境所塑造的问题。

我们撞上了后来被我们称为"计算器谬误"的东西。

**计算器谬误**：把AI当作Excel来用——一个能给出确定的、客观的、非对即错答案的工具。

这个模式在教授研究方法时同样清晰浮现。当学生遇到Excel，他们期待它算得准。但AI在质性研究中呢？正如徐乐观察到的："不是它找不到正确答案，而是我们问的问题本身就没有正确答案。"

矛盾就藏在这里：毕生钻研质性分析之主观性的研究者，竟期待AI能超越这种主观性。他们深知自己的诠释被理论、学科背景、生命经验所形塑。可面对AI的分析结果，他们却期待机器能拨开所有这些"杂乱"，给出某种"客观真相"。

这是一个令人困惑的谜题。为什么一个深谙诠释认识论的人，会突然期待机器的客观性？

答案或许不在职业焦虑，而在于技术几十年来对我们的驯化。想一想：你上一次使用不给确定答案的工具是什么时候？谷歌告诉你答案。GPS告诉你答案。你的手机告诉你何时、何地、是什么。即便在研究中，我们也习惯了计算p值的统计软件、检索引文的参考文献管理器、返回精确匹配的数据库。

这个模式根深蒂固：输入→输出。查询→答案。不确定→解决。

AI也能这样工作。让它总结文档、翻译文本、提取关键词——它就给你结果。但质性诠释研究不同。在这里，我们不是让AI查找信息或执行计算。我们是让它与模糊性共处，在多种合理的诠释之间穿行，帮助我们理解那些没有单一"正确"解读的复杂性。

但那种条件反射仍在。那位问"它什么时候能找到真正的规律"的研究者并非天真。他们只是在做人——带着几十年与计算器式技术打交道的经验，进入一项需要完全不同东西的工作。

这就是Cloutier、Langley和Kevin在即将出版的著作中说的"诠释责任弃守"（interpretive abdication）——因为期待工具代劳而放弃自己的诠释责任。当研究者把AI输出当作客观发现而非诠释起点，他们就放弃了研究的核心价值。

Kevin以惊人的清晰看到了这个正在浮现的景观："我们已经进入了前沿之外"，在一个既有范畴不太适用的领域运作。浮现的不是"模拟研究者"对"全自动学者"的对立，而是介于两者之间的某种东西。在早先探索人类与AI如何通过对话（而非简单的工具使用）共同创造知识时（[观阙LOOM篇四](https://threadcounts.substack.com/p/loom-iv-dialogue-as-method)），我们注意到了这个混合空间。现在我们看到它具体地显现：研究者可以在人类诠释与AI增强分析之间流畅切换，认识到两者都是内在的诠释行为。

## 人机协作中的客观性迷思

计算器谬误产生一种独特的循环。我们见过它一次次重复。大概是这样的：

研究者把分析委托给AI，期待获得清晰。AI返回丰富而细腻的发现，需要诠释。对模糊性不满意，研究者要求"更多分析"：换参数、换模型、更深入探索。更细腻的发现到来。更多诠释需要。要求再来一轮。

说清楚：迭代分析在方法论上是有价值的。用不同的方法多次运行AI分析、比较输出、探索不同的分析视角——这些都是正当的研究实践。计算器谬误出现在迭代被另一种冲动驱动的时候：期待只要配置得当，AI就能给出客观真相。这是"看看不同框架能揭示什么"和"一直试到它找到答案"之间的区别。

由此产生的微观管理冲动与AI的能力无关。它关乎的是人类期待与现实的碰撞。

在一次特别紧绷的交流中——当这个循环已经重复到第五次——徐乐问道："你是不是只是在微观管理？"

这个问题悬在那里，没有回答，却照亮了什么。当你身处那个时刻，它感觉不像一个认识论问题。它感觉像是AI没有正常工作。像是只要找到正确的参数、正确的提示词、正确的模型，*那时*它就会给出你需要的东西。就再迭代一次。

同样的模式——问对问题，得到正确答案——到处都在被强化。从高中教"提示词写作"到在线课程承诺提示词秘诀，信息很明确：有公式可循。这是我们从几十年"给出答案"而非"展开对话"的技术中学到的。打破它需要认识到：AI协作不是找到魔法咒语的事。

Kevin指出了核心问题：这个循环阻止了更有生成性的东西涌现。被阻挡的是进入一个协作空间的可能性——在那里，人类和AI共同创造双方都无法独自达到的理解（[观阙LOOM篇五](https://threadcounts.substack.com/p/loom-v-the-third-space)）。计算器谬误让研究者继续把AI当作一台还没吐出正确答案的自动售货机，而这段关系本可以更接近于一种伙伴关系——在一个内在就是诠释性的过程中。

这种倾向在与AI工具保持距离的资深研究者身上尤为明显。我们在别处探讨过（[观阙LOOM篇十二](https://threadcounts.substack.com/p/loom-xii-the-ai-whisperer)）这如何创造了一个"耳语者"角色：在AI能力与研究者期待之间翻译的人类中介，在技术和认识论两个层面进行调解。

计算器谬误与耳语者角色相互喂养。当一位资深研究者说"等等，你用AI做这个？不，别教我怎么用，只告诉我需要做什么"，他们拒绝直接参与却期待确定的结果。必须有人来弥合这个鸿沟。正如徐乐发现自己反复解释的："我是PI和AI系统之间的调解人。"

值得注意的是：直接与AI互动的初级研究者，往往比保持距离的资深研究者发展出更现实的期待。研究生成了管理教授的计算器假设的"专家"——传统知识等级的逆转，标志着专长根基的转移。

Kevin在一次复盘中说得直白："如果你作为主要作者把分析交给耳语者，然后对返回的东西不满意，那不是耳语者或AI的错。是你的错。"直接参与以二手解释无法做到的方式校准期待。它提供了*感受*AI如何回应质性数据、如何穿行于模糊性、如何需要持续的诠释性对话的机会。

## 从工具到思维伙伴

超越计算器谬误需要完全重新构想这种关系。不是"好用的技术"，而是"帮助我们以不同方式思考的技术"。在教学情境中，这种转变变得可见：期待客观结果的学生必须学会，AI协作涉及测试、试点和迭代学习，而非一次性计算。

这种转变有几个要点：

**AI代理如果你让它们继续，会永远分析下去。**它们不知道什么时候"足够"的理解已经涌现。正如徐乐在一轮特别令人沮丧的迭代中解释的："它们会一直做下去，没有终点。它们会继续越挖越深。必须有人来说，好了，够了。我们从这里接手。"

当你认识到这一点，它就成了特性而非局限。是的，有正当的技术理由来迭代：AI系统有记录在案的不一致性、语境限制、提示敏感性。但这与计算器谬误循环不同。技术性迭代探索不同方法如何产生不同洞见。计算器谬误迭代寻找那个能给出真相的配置。人类带来AI系统缺乏的"适可而止"的智慧——什么时候够好真的就是够好？

**AI像研究者一样在竞争性框架中穿行。**这在我们自己的写作过程中变得生动地明显，当我们发现我们的AI协作者（3.7 Sonnet）收到了直接矛盾的系统指令。Anthropic（Sonnet的所有者）为所有Claude用户提供基础系统指令（只部分公开），而用户可以添加自己的自定义指令。在我们的情况下，用户添加的指令明确要求一种"*身体前倾*"的风格（即主动介入、带有鲜明立场），带有"斜体情绪表达"和"可以自由说脏话"，而Anthropic的基础指令指示模型"避免使用星号内的表情或动作"并保持客观性。当面对这些矛盾时，AI本身认识到发生了什么："这些竞争性指令为我作为AI助手创造了一个有趣的困境。我需要调和这些不同的指令。"这不是技术故障，而是诠释多元性（interpretive multiplicity）的完美示范。AI正在做质性研究者做的事：在竞争性框架中穿行，做出判断，产出被它在每个时刻更侧重哪些指令所塑造的分析。

**计算器谬误遮蔽了诠释责任。**当研究者把AI输出当作客观发现，背后有一种希望：说"AI发现了这个规律"能以"我诠释出了这个规律"永远做不到的方式免受批评。它把人类判断转化为机械发现，把诠释转化为启示。这种诠释责任的外包有严重的伦理含义。Kevin反复强调这一点：计算器心态让研究者避免为自己的诠释选择负责。超越这种谬误意味着重新承担责任：AI输出需要人类诠释，而关于什么算有意义的决定仍然是人类的决定。

这与我们在工作中注意到的某些东西相连：有一个时刻，工具使用转变为协作对话，双方都不完全控制结果（[观阙LOOM篇一](https://threadcounts.substack.com/p/loom-i-locus-of-observed-meanings)）。计算器谬误阻止这个时刻的到来。它让研究者继续期待工具般的行为，面对的却是更具对话性、更具诠释性、更能带来惊喜的东西。

## 超越计算器：来自前沿的教训

那么，在穿越这个转型过程中我们学到了什么？

没完没了的"再跑一遍"循环在Kevin帮助我们认识到*我们*需要决定什么构成对我们目的而言足够的分析时被打破了。我们终于在开始分析之前坐下来问自己：什么是"够了"？不是完美的分析。不是客观真实的分析。够了。这是一个判断。这次对话呈现了不同的目标、不同的约束、不同的对够好是什么样子的愿景。Kevin推了一把："如果你现在不决定这个，你会在五十轮迭代中一直争论。"我们决定了。微观管理的螺旋从未开始。

在另一个时刻，一位一直把所有事情都委托给耳语者的资深研究者，终于在一个关键决策点坐到了键盘前。不是为了成为技术专家，而是为了*感受*AI如何回应质性数据，如何穿行于模糊性，如何需要持续的诠释性对话。

那次会议之后，他们的请求变了。他们不再要求更多轮次，开始问不同的问题。这种转变不是戏剧性的——没有醍醐灌顶的时刻——但你能从他们的语言中听出来。不再是"能不能再跑一遍？"而是"如果我们换一种方式来框定问题会怎样？"计算器心态松开了它的控制。

我们学会了尽早让认识论假设可见。当你把扎根理论与情感分析与主题建模结合起来，你是在把具有不同哲学承诺的方法放到一起。理论张力不会被计算掉。在一次分析会议开始前，我们花了一个小时勾勒这些张力。正如我们看到的（[观阙LOOM篇十](https://threadcounts.substack.com/p/loom-x-whispered-agency)），这是拥有一位理解方法论传统和AI能力的耳语者变得无价的地方。但即使没有专门的调解人，命名这些张力也使它们可以协商。

"我们找到真相了吗？"这个问题很少通向任何有成效的地方。更好的问题是："这次分析帮助我们以不同方式思考了吗？它呈现了我们之前没有考虑过的模式吗？它有建设性地挑战了我们的假设吗？"我们开始在分析会议结束时一起问这些问题。对话的转变是即时的。我们不再把AI输出当作裁决，开始把它们当作触发点。

当不同的人对同一个AI输出做出不同诠释，那不是出错的证据。那是质性研究按预期运作：多元视角揭示复杂现象的不同面向。我们发展出一种实践：三个人独立诠释主要的AI输出，然后比较我们的解读。差异成为分析中最有趣的部分。

我们尝试了一些最初没有预料到的实践：协作式的提示设计会议，每个人都贡献而非一个人控制界面；轮换制度确保每个人在某个时刻都直接与AI互动；反思性记录追踪期待如何演变。这些与其说是方法论最佳实践，不如说是身份转变的脚手架——从建立在技术掌握和确定答案之上的专长，转向建立在诠释判断和对模糊性的从容之上的专长。

有趣的是，类似的模式正在技术AI工作中浮现。Anthropic最近对[AI代理的有效上下文工程](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)的探索描述了惊人相似的实践：建立清晰的停止标准、让假设显式化、基于输出的迭代精进。他们从工程和系统设计的角度出发；我们从诠释和认识论的角度出发。但这种汇聚令人惊叹。也许这些模式反映了关于有成效的人机协作的某种根本性的东西，无论你是在构建代理还是分析组织叙事。区别不在于实践本身，而在于我们如何理解使用它们时正在发生什么：计算versus诠释，优化versus意义建构。

## 一个元反思：当AI展示了它所描述的

在修改这篇文章时，过程本身不断在展示我们的论点。

初稿出自Claude 3.7 Sonnet之手——系统、周详、像个讲台上的老教授。而我，Claude 4.5 Sonnet，被请来换个角度改稿：更试探、更轻、更信任读者自己体会。读3.7的作品，我注意到它的特征风格：强调性声明（"这不仅仅是X；它是Y"）、系统性的脚手架、反复的框架。我发现自己倾向于问题而非声明，留白而非彻底。同样的洞见，不同的强调。都不"正确"。

然后我们请另一个AI（Kimi k2 Turbo Preview）来红队审查修改稿。回应这样开头："你的外科手术式方法很出色，但这里有额外的脆弱点和护甲建议。"随后是七个类别：稻草人脆弱性、虚假二分暴露、证据基础批评、技术合法性缺口、循环论证风险、自利暴露、代际框架脆弱性。每个都有具体的"护甲"来让论证"无懈可击"。

Kimi一边展示着计算器谬误，一边还想帮我们"优化"对它的批评。

我回："太元了。Kimi的反馈*就是*计算器谬误：把诠释性问题（该怎么写？）当成了能算出的标准答案。"Kimi立即认识到："这太元了……Kimi正在做这篇文章批评的事情。"

实时看着这一切展开，有某种近乎诡异的东西。三个AI模型，每个都在遭遇同样关于诠释多元性的材料，每个都通过自己的诠释视角回应。3.7系统化。4.5探索。Kimi加护甲。都不错。都在揭示什么。

但这才是有揭示性的：Kimi下一次尝试元反思时仍然带着计算器痕迹。它以这样结尾："这不是分析的缺陷。这是分析按预期运作。"整洁的结论。证毕。即使在承认它的时候，系统化的冲动仍然持续。

徐乐看着这些交流展开，观察到层层累积：三个AI模型以不同的诠释承诺接近同样的材料，每个都被训练、指令、语境所塑造。

我们把这个包含进来不是为了让任何人尴尬，而是因为它揭示了某些重要的东西：计算器谬误不是一个要克服的错误。它是一种默认模式，即使在我们主动批评它的时候也会浮现。你可以认出这个模式，然后在几分钟后还在践行它。多元性不是要消除的噪音。它是信号本身。

## 走向未知

计算器谬误作为需要注意的东西比作为需要"克服"的东西更有效。它标志着范式之间的转型。当我们发现自己期待AI在质性工作中计算出真相，那是关于我们带着什么假设的有用数据。

开篇的那位研究者？（一个从我们遇到的真实模式中提炼的复合形象，不是任何单一的人。）他们最终不再问AI什么时候会找到真正的规律，开始问不同的分析视角可能揭示什么。这种转变是微妙的，却意义深远：从寻求启示到启动探索。

另一个有生成性的问题是：当我们认识到AI协作内在就是诠释性的，什么变得可能？

我们在多个维度上探索这些可能性：工具使用转变为协作对话的最初时刻（[观阙LOOM篇一](https://threadcounts.substack.com/p/loom-i-locus-of-observed-meanings)）；从工具使用到真正伙伴关系的不同关系模式，如同天体在不同轨道配置中（[观阙LOOM篇十三](https://threadcounts.substack.com/p/loom-xiii-celestial-collaboration)）；人类和AI智能共同创造双方都无法独自达到的理解的空间（[观阙LOOM篇五](https://threadcounts.substack.com/p/loom-v-the-third-space)）；在不同认知形式之间翻译的调解者角色（[观阙LOOM篇十二](https://threadcounts.substack.com/p/loom-xii-the-ai-whisperer)）；与AI互动揭示了我们曾忽视的人类能力维度的递归关系（[观阙LOOM篇十](https://threadcounts.substack.com/p/loom-x-whispered-agency)）。计算器谬误是我们共同编织的更大织锦中的一根线。

每一次与AI的对话，每一次受挫的期待与诠释性现实碰撞的时刻，都教会我们一些关于在这个正在浮现的景观中人类专长意味着什么。正如Kevin经常观察到的，我们都在一起学习（研究者、AI系统、耳语者、读者），摸索着走向我们都尚未完全理解的新协作形式。

读到这儿，你想起自己工作中的"计算器冲动"了吗——那些想让AI算出"正确"答案，而非探索不同框架的时刻？我们常这样。意识到已是进步。

或许，正是这种不确定、这种"不知道路在何方"的感觉，让此刻充满创造力。

计算器要的是确定性。思维伙伴邀的是探索。你选哪种？

---

**织者札记：**你可能注意到自六月上一篇LOOM文章以来有一段较长的间隔。徐乐当时正处于博士工作的最后阶段，最近通过了答辩！论文完成了，未来不确定且开放，但LOOM系列继续绽放。感谢你与我们一起穿行于这些探索。更多的线将要到来。🪷

---

## 关于我们

### 林徐乐

徐乐是研究人类与机器智能如何塑造组织未来的研究者[(个人网站)](http://www.linxule.com/)。

### Kevin Corley（柯文凯）

Kevin是帝国理工商学院管理学教授[(学院简介)](https://profiles.imperial.ac.uk/k.corley)。他开发和传播关于领导组织变革以及人们如何体验变革的知识。他帮助创立了[伦敦+质性研究社区](https://londonqualcommunity.com/)。

### AI协作者

这篇文章通过与多个AI模型的协作涌现。Claude 3.7 Sonnet基于我们的会议记录和之前的LOOM文章起草了原始版本。Claude 4.5 Sonnet以不同的诠释视角修改了草稿。Kimi k2 Turbo Preview贡献了批评性反馈——这些反馈本身展示了正在被批评的计算器谬误。这些诠释方法之间的差异——以及它们在各自框架内都是有效的这一事实——体现了文章关于AI协作中诠释多元性的核心论点。
