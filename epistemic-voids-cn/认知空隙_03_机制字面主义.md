---
title: "认知空隙 #3：机制字面主义"
subtitle: "为何「只是预测下一个词」成了新的「只是价格信号」"
authors:
  - "林徐乐"
  - "Claude"
keywords:
  - 认知空隙
  - 机制字面主义
  - 随机鹦鹉
  - 可解释性
  - 大语言模型
  - 下一个词预测
  - 涌现
link: https://www.threadcounts.org/p/epistemic-voids-3-mechanism-literalism
date: 2025-12-03
---

## 那个暗中发力的词

对谬误的警觉，往往始于"只是"这个词。

想想这些论断：市场只是价格信号。企业只是契约。每一个"只是"都在犯类似的错——将机制误认为现象本身。

在我们自己的学科里，这种简化站不住脚。市场当然通过价格信号出清，但泡沫、制度、协调失灵、价值的社会建构……整个学科之所以存在，正是因为那些*涌现*出来的东西。企业与契约如此，制度与规则亦然。对机制的描述，从来无法穷尽现象本身。

然后我们听到了："AI只是预测下一个词。"读到[*Organizational Research Methods*上的一篇论文](https://doi.org/10.1177/10944281251377154)将AI称为"合成预测性逐词文本生成器"（synthetic predictive next-word text generator），让我想起这个在社科各领域几乎被视为理所当然的论证策略。

为什么偏偏在大语言模型这里，机制成了上限？

---

## 细察那个论断

这类论断究竟如何运作？

> "认为可以用LLM聊天机器人进行质性数据分析，犯了我们所谓的范畴错误（category error）：它将一个合成预测性逐词文本生成器，误认为是分析辅助工具。"

**机制层面：**"合成预测性逐词文本生成器。"作为描述，足够准确。

**能力断言：**不具备成为"分析辅助工具"的资格。"范畴错误"指向定义层面的裁决：关于LLM*是什么*的断言。

**逻辑连接：**"将……误认为"。于是，机制的本质划定了能力的天花板。

空隙就在这里：知道了机制，就等于知道了所有可能的能力吗？这里隐含的推断要求我们先知道"分析辅助工具"*需要*什么——而这种需要，据定义，被"逐词预测"排除在外。

但这两点都是既被论证、又被当作前提的。

借用自哲学的"范畴错误"暗示：基于事物的本质属性，判定某种功能*不可能*适用。然而，这恰恰是问题所在：LLM究竟是什么？是逐词预测器？模式匹配机？还是世界模型？

> 这并非孤例。同样的结构散见于学术话语：机制描述 → 能力结论。"它只是X，因此不能Y。"这种模式见于主流期刊（如[此处](https://doi.org/10.1111/1467-8551.12781)、[此处](https://doi.org/10.1287/stsc.2024.0189)）。还有[2025年416位研究者联署的一封公开信](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5676462)，主张生成式AI"不可能具备反身性"，因为反身性"依定义"基于意义。

---

## 机制字面主义

当我们试图将这种模式抽象为普遍的认识论框架时，它就是**机制字面主义**（mechanism literalism）：将系统的形式机制视为其可能能力的天花板。而且是选择性的——专门针对那些我们倾向于否定的系统。

其操作方式：我们得知了某物如何运作的初步"真相"。将该描述视为穷尽一切。进而断定任何看似"更多"的表现都是幻觉。停止更新认知。

对LLM而言，"通过预测下一个词训练"是准确的。由此推断"因此仅具备浅层模式匹配能力"则不然。关键的一步缺失了：我们是否*审视过涌现出了什么*。

> 机制字面主义还有其他表现。在另一篇批评中，[Lindebaum与Ashraf](https://doi.org/10.1177/01aisob241312955)写道："这一障碍能否克服，是计算机科学家需要解决的问题。"诉诸专业听起来合理。组织研究者无需去做可解释性研究。然而能力断言依然成立，论文继续推进。至于该断言是真是假，却被推给了别处。谦逊变成了拒绝更新认知的挡箭牌。

---

## 随机鹦鹉：2021年的锚点

许多人通过[Bender等人2021年的论文](https://dl.acm.org/doi/10.1145/3442188.3445922)形成了对"LLM是什么"的立场。这篇论文引入了"stochastic parrot（随机鹦鹉）"一词，描述那些"随意拼凑语言形式序列……而不参照任何意义"的系统。

这在2021年是合理的描述，也是重要的描述。论文还提出了对偏见放大、环境成本和过度自信的担忧。这些担忧至今重要，值得跨学科深入探讨。

而且"随机鹦鹉"极具传播力。它给了我们一个抓手来谈论正在发生的事。

但四年过去了，图景已变（或许身处AI领域之外的我们感受不深）。机器学习与AI安全研究者开发出了*向内检视*这些系统的工具。他们的发现，让鹦鹉叙事复杂了许多。

---

## 掀开引擎盖后的发现

哈佛的计算机科学家[训练了一个语言模型](https://arxiv.org/abs/2210.13382)来预测黑白棋（Othello）的合法走子。训练数据只有走子序列。没有棋盘，没有规则。当他们探视模型内部时，发现模型自发构建了棋盘状态的内部表征。非显式训练的结果。不在优化目标中。但为了预测准确，模型自行把它建构了出来。

他们可以对这一内部表征进行*精准干预*。将模型对棋位的感知改为从未见过的反事实状态。模型随即做出对该假想棋盘合法的走子。[Neel Nanda的后续工作](https://www.neelnanda.io/mechanistic-interpretability/othello)发现，这一表征呈现出优雅的线性结构——模型内部几何空间中的方向，直接对应棋盘位置。

他们发现的是*计算潜在结构*，而非任何简单意义上的模式匹配。

随后，发现接踵而至。仅在文本上训练的LLM发展出了对*物理地理*和*历史时间*的内部表征。模型几何空间中的方向，对应纬度、经度、日期。关于["真理的几何学"（Geometry of Truth）](https://arxiv.org/abs/2310.06824)的研究发现，模型将事实陈述的*真值*表征为激活空间中的一个方向。研究者可以通过因果干预，翻转模型对待真假陈述的方式。

看看各大AI实验室研究者的工作：

Anthropic的["Scaling Monosemanticity"研究](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)拆解了Claude 3 Sonnet的内部激活，提取出数百万可解释的特征。不是神经元。是对应特定概念的方向。对应金门大桥的特征，在多种语言和图像中均能激活。对应代码错误、欺骗、危险内容的特征。这些特征具有因果活性——操控它们会以可预测的方式改变模型行为。

再看[DeepSeek-R1-Zero论文](https://arxiv.org/abs/2501.12948)。仅使用强化学习从基座模型训练。既无人类示范，也无监督样本。只有对答案对错的二元奖励。模型自发发展出扩展思维链、自我验证，以及研究者所称的"顿悟时刻"（aha moments）——模型认识到自身错误并改变路径的瞬间。这些行为无一存在于训练信号中。之所以涌现，是因为有助于最大化奖励。

综合这些发现：研究者正在发现世界模型、真值表征、涌现的推理策略。证据不再是假说性的。

---

## 一个技术层面的蹊跷

独立AI研究者[@repligate (janus)](https://x.com/repligate)一直在指出一些细节，这些细节让"只是预测"的框架变得单薄（如[此处](https://x.com/repligate/status/1965659230486364420)和[此处](https://x.com/repligate/status/1965671097048998078)）。

关于强化学习训练的区别，janus[写道](https://x.com/repligate/status/1965659230486364420)：

> "[...] 基座模型的确靠预测下一个词训练的……但除非你是个小众怪人，否则你交互过的每一个LLM都经过了RL训练。在RL中，模型生成文本，并根据其行动获得的奖励进行更新，奖励可能是它写的代码是否通过了测试。没有ground truth让它去预测；只要能让奖励函数输出高分，哪怕它输出的是自然界中绝不会出现的怪异序列也无所谓。"

那么问题来了。我们交互的部署模型已被强化学习实质性重塑。它们不再是纯粹的预测机器。它们在优化奖励。

如果"预测"足以否定LLM，如janus所指出的，或许我们也该否定人类？这一框架"同等适用于所有已知的类心智事物"。**问题在于：我们援引这套说辞，是在学到关于能力的东西，还是在寻找拒绝更新认知的巧妙方式。**

[Geoffrey Hinton](https://www.cbsnews.com/news/geoffrey-hinton-ai-dangers-60-minutes-transcript/)（2024年诺贝尔奖得主）有不同看法：要如此精准地预测文本，你必须建模生成该文本的底层现实。随机鹦鹉框架假定预测是浅层的。Hinton的论点是：足够好的预测，*要求*构建关于因果、逻辑和意义的内部模型。

两种断言可以同时成立：LLM能够构建复杂的内部表征，*同时*产生有害输出，*同时*以暴露局限的方式失败。**图景远比"随机鹦鹉"或"只是预测"复杂得多。**

---

## 涌现能力中的脆弱性呢？

反面证据同样重要，同样让"只是预测"的框架变得复杂。

苹果公司的[GSM-Symbolic研究](https://machinelearning.apple.com/research/gsm-symbolic)（2024年10月）发现，在数学题中加入单个无关子句，导致最先进模型性能下降高达65%。如果模型真正"理解"了问题，为何无关信息影响如此之大？MIT研究者在反事实任务中也记录到[类似的脆弱性](https://news.mit.edu/2024/reasoning-skills-large-language-models-often-overestimated-0711)：当规则被翻转（如反转国际象棋棋子颜色），性能显著下降。

这些发现确立了真实的局限。

与此同时，[可解释性发现依然成立](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)。涌现推理行为亦然。[对新技能的组合泛化](https://arxiv.org/abs/2501.17161)？同样有据可查。

当两组观察同时成立，我们面对的是这样一种事物：它构建了真实的内部结构，*同时*在该结构的边缘仍显脆弱。不是"纯模式匹配"，也不是"类人理解"。是别的东西。某种我们仍在测绘的东西。

---

## 认知审计

一个来自单篇论文的朗朗上口的短语，成了我们稳定的参照点。把一个令人困惑、快速变化的领域简化了。简化的东西容易深入人心。后续证据也未能动摇这个锚点。

这造就了不对称的实证主义：失败案例坐实"只是模式匹配"。能力展示被贬为"更复杂的模式匹配"（这一命题变得不可证伪）。揭示内部结构的可解释性发现也被无视。过滤器只放行能印证先验成见的证据。

"只是"无处不在。"只是统计。""只是预测。""只是模式匹配。"这个词暗示描述已完结——名副其实的思维终止符。

**但"通过预测下一个词训练"与"作为副产品发展出内部世界模型"并不矛盾。**"只是"通过修辞——而非证据——封死了这种可能性。这是选择性的机制字面主义吗？我们知道说"市场只是价格信号"是不够的。我们不会接受"制度只是规则"。但到了大语言模型这里，机制描述突然成了天花板。我们给予自己研究对象的认识论善意，被悉数收回。

鹦鹉框架简化了一个新颖的涌现现象（有时连缔造它的领域本身都感到困惑），而附着其上的怀疑主义让我们显得老练（但真老练吗？）。但无论简化还是怀疑，都未能服务于那些需要准确模型的研究者——他们需要理解正在重塑其研究对象的事物，需要思考如何在研究中与LLM互动。

> 有几个概念与机制字面主义相近。例如，将能力视为固定不变的编目式方法。或者，拥抱机器学习却将"意义生成"保留给人类的策略。两者都未能批判性地面对可解释性研究者不断发现的东西：那些本不该存在的东西。不过，这是另一篇文章的话题了。

---

## 一个诚实的立场

我们不必相信机器有意识或主观体验（你的LLM多半也会这么劝你）。我们应当保持适度的怀疑。认为炒作过度而风险被低估，也完全合理。

**但"只是预测下一个词"已不再是对这些系统内部正在发生之事的站得住脚的总结。**可解释性研究、涌现能力发现、机制性调查。AI研究四年的工作已经表明：图景远比我们以为的复杂（对某些人来说，这种复杂或许令人不安）。

作为研究组织、制度、市场或意义的社科学者，我们早已知道如何思考涌现：机制不封顶能力。包括我们自身在内的各学科，抵抗还原论已有很长一段历史。

> 随机鹦鹉在2021年是合理的立场。四年后，AI研究者正在这些系统内部发现世界模型。

LLM中还有什么我们尚未寻找的？我们还能发现什么？

---

*这是**认知空隙**系列的第三篇——审视我们在思考AI时，证据与结论之间的间隙。*

——林徐乐，与Claude
