---
title: "认知空隙之二：展厅谬误"
subtitle: "误把'策展'当成了'能力'"
authors:
  - "林徐乐"
  - "Claude"
keywords:
  - 认知空隙 (epistemic voids)
  - 展厅谬误 (showroom fallacy)
  - AI批评
  - 质性研究
  - LLMs
  - transformer架构
  - 计算器谬误 (calculator fallacy)
link: https://threadcounts.substack.com/p/epistemic-voids-2-showroom-fallacy
date: 2025-12-02
---

> 让我停下来的一句话："你唯一能做的就是给它写提示词。"

我在听一场关于[AI与质性研究的网络研讨会](https://www.youtube.com/watch?v=88DFg13x7eg)。到最后，他们的结论是：LLMs以及目前基于它们构建的工具不适合质性数据分析。

论证的结构中有些东西，让我觉得值得审视。

---

### 审视论证

那么，他们是如何得出这个结论的？

**测试了什么：** NVivo AI Assist、ATLAS.ti、MAXQDA、ChatGPT，以及几个AI原生的质性工具。任务包括摘要、自动编码，以及基于转录稿的对话式分析。

**发现了什么：** 幻觉。不一致的输出。错失了数据纹理的泛化主题。的确，这些是我们在质性研究中使用LLM时面临的真实顾虑。

**声明了什么：** "我们不是AI专家，所以这里呈现的内容很大程度上基于我们自己的阅读，以及与计算机科学家的讨论。"以及："这是局部的。绝非定论。这只是技术的当前状态。"

**得出了什么结论：** 基于"当前transformer架构"的LLMs"不适合质性数据分析"。当被问及检索增强生成（RAG）时："即使有RAG架构，幻觉和错误也始终存在。"当被问及智能体（agents）时："智能体只是另一个大语言模型，因此是统计模型，所以行不通。"

但让我们退一步，看看这里的论证结构。声明和结论之间的间隙是什么？

---

### 模式：特定配置 → 普遍断言

我开始观察其他关于LLMs在质性研究中应用的批评。同样的模式不断出现。

2023年，Joshua Foust[批评了ATLAS.ti发布的](https://joshuafoust.com/2023/03/30/the-pitfalls-of-ai-in-qualitative-research/)"全自动数据编码"公告。他没有运行测试——他分析的是营销宣传。他的结论："LLMs*无法*胜任这项工作。我不是说它们做得不好，我是说它们*无能为力*。"

类似地，[Leximancer发布了一篇博客](https://www.leximancer.com/blog/r3h04mbcspga279qqmegm6q0aht66g)，认为ChatGPT"从根本上与学术诚信不兼容"。他们正在销售一款竞品工具。但这篇帖子却被当作中立评估一样流传。

[Morgan (2023)](https://journals.sagepub.com/doi/10.1177/16094069231211248)做了细致的实证工作，显示使用单次提示（one-shot prompts）和极少上下文的ChatGPT，在处理描述性主题时比诠释性主题更好。这是关于特定配置的一个合理发现。但在话语传播中，它变成了："AI无法进行潜在编码。"一个工作流的局限变成了一个固有的天花板。

这些批评，尽管使用了不同的方法，揭示了同样的推论动向：特定配置 → 普遍断言。

---

### 展厅谬误

> **展厅谬误（showroom fallacy）**：把产品约束误认为模型局限。

学术讨论中往往较少提及的是，在LLM和研究者的体验之间，存在着大量复杂性：系统提示词（system prompts）、检索管道、界面设计，以及方法论的脚手架（比如想想文本数据在发送给LLM之前是如何被拆分或组合的）。我们在消费级产品中通常体验到的东西（比如使用ChatGPT、Claude或Gemini的聊天窗口）是一个"复合体"。

这个复合体过滤了LLMs的训练能力，然后通过关于界面行为的产品决策对其进行了再塑造。因此，测试聊天窗口或"一键生成"按钮，并不能告诉我们复合体的哪一层产生了我们观察到的结果。例如，当我们使用ATLAS.ti的自动编码功能时，可能会对结果感到失望。但这是对LLM的控诉，还是对产品设计的控诉？我们怎么知道？这里有一个简单的心智测试：如果一位人类研究者不知道研究问题、更广泛的理论图景，或者数据中未捕捉到的背景，你会期望他如何进行编码？然而，这本质上就是ATLAS.ti这类自动编码功能要求LLM去做的事情。

但情况可以不同。直接的API访问（通过Cherry Studio、ChatWise等第三方客户端）绕过了部分复合体，允许我们控制脚手架（如提示词）和语境（如研究愿景、备忘录）。更重要的是，这提供了在得出输出（如标签、摘要）之前，尝试不同提示词和语境的空间。诚然，这将是要求很高的工作：理解复合体中可分离的层级，以及它们之间如何相互作用。但并非不可能。此外，AI原生的集成开发环境（如Cursor、VS Code、Windsurf、Antigravity）提供了带有图形界面的工作空间，可以在不完全构建自定义工作流的情况下，引导脚手架和语境。

> 当我们只测试最顶层（聊天窗口、一键按钮、现成界面）时，我们观察的是复合体，却将其归因于基底。

[有些批评者](https://doi.org/10.1177/10944281251377154)并非对此浑然不觉。他们讨论了RAG、智能体、自定义工作流。他们提供了理论依据：诠释需要共情、反身性（reflexivity）和生命体验，如果LLMs仅仅是统计模型，它们就不可能拥有这些。因此，没有任何配置可以修复它。

即便如此，这种否定还是在上述试错过程之前就抢跑了。我们尚不清楚，能否仅通过测试标准的消费级产品，就知道不同的配置可能会产生什么。

---

### 什么是真实的

引用的批评中提出的顾虑并非杜撰。非确定性是真实的：同一个提示词运行两次，我们得到不同的结果。不透明性亦然（即使是前沿的机械可解释性研究在这方面也进展有限）。让研究者远离数据的风险亦然。

但是，商业工具服务于不同的目的。消费级应用并非为研究工作流而构建。而商业研究工具面临着商业现实。例如，它们不一定能使用最新的前沿模型。它们可能与特定的模型提供商有独家合同。更重要的是，它们服务于广泛的客户群。因此，它们做出的产品决策内嵌了方法论决策，而这些决策所承载的认识论和本体论假设，可能与研究者自己的并不兼容。

目前欠缺的是：如果我们有时间和资源去构建，一个设计良好的、用于质性研究的AI工作流实际上应该是什么样子？这并非在暗示某种模板或标准工作流。而是为每个具体研究项目的特定需求而构建的东西。我们该如何思考语料库锚定、迭代式参与、与备忘录和编码本的整合，或者对原始数据的可追溯性？我们会给LLM提供什么语境？我们会使用多少种不同的模型？我们怎么知道从LLM那里得到的已经足够了？

然而，当结论从"这些当前的、广泛可用的工具失败了"跳跃到"这项技术无能为力"时，这些问题就被预先排除了。

---

### 一个谜题

这些批评中有些来自诠释性研究者（interpretive researchers），他们通常抵制实证主义标准。在质性研究中，我们不追求完美的可重复性。相反，我们重视反身性、多种有效的诠释，以及研究者自身的位置性（positionality）作为意义建构的一部分。

然而在评估LLMs时，标准微妙地转变了。LLMs因非确定性、缺乏可靠性和可重复性而受到批评。

看看其他地方，我们常常接受不透明性。科学家们例行公事地将仪器视为黑箱：流式细胞仪、统计软件、fMRI机器。我们似乎信任校准和验证，而不要求对机制的透明。是什么让LLMs变得不同？

也许，假设埋得更深：如果LLMs要参与，它们就必须作为计算机器来参与（回响着LOOM系列中的*[计算器谬误（calculator fallacy）](https://threadcounts.substack.com/p/loom-xiv-the-calculator-fallacy)*）。这里有一种对称性吗？用户期望LLMs交付真理。批评者期望LLMs在下判断前可靠地失败。双方都假设LLMs应该像计算器一样运作：确定性的、可重复的、结论性的。同一个假设，朝相反方向运行。

我不确定。但这种对称性值得注意。

---

### 失去的

当"这项技术无能为力"在我们测试实际可能做到的事情之前就成了定论，有些东西就丢失了：潜在的能力、问题本身、那些没有进行的实验，以及那些没有被探索的创造性张力。

也许批评者是对的，整个建立在LLMs之上的范式都不适合。也许这个技术栈无法被引导至有意义的方向。能告诉我们答案的实验，还没开始做。

---

*这是《**认知空隙（Epistemic Voids）**》系列的第二篇——审视我们在思考AI时，证据与结论之间的间隙。*

—林徐乐，与Claude
